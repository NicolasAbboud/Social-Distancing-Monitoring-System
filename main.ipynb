{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e910a88",
   "metadata": {},
   "source": [
    "# Social Distancing and Mask Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41174a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np \n",
    "import face_detection\n",
    "from tensorflow.keras.models import load_model\n",
    "from focal_loss import BinaryFocalLoss\n",
    "from scipy.spatial import distance as dist\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as pre_mobile\n",
    "from face_detection.dsfd.detect import DSFDDetector\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c113aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained mask detector model\n",
    "mask_detector = load_model(\"models/mask_detector_model.keras\", custom_objects=None, compile=True)\n",
    "\n",
    "# Initialize the YOLO model\n",
    "model = YOLO(\"models/yolov8n.pt\")\n",
    "\n",
    "# Import names of the classes\n",
    "classesFile = \"other/coco.names\"\n",
    "\n",
    "# Read the class names from the file and store them in a list\n",
    "classes = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f3a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DSFD face detector with specified parameters\n",
    "detector = DSFDDetector(confidence_threshold=0.6, \n",
    "                        nms_iou_threshold=0.5, \n",
    "                        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                        max_resolution=1080, \n",
    "                        fp16_inference=False,\n",
    "                        clip_boxes=True)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(\"./media/testvideo.mp4\")\n",
    "\n",
    "# Initialize Output Video Stream\n",
    "video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "video_n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "out_stream = cv2.VideoWriter(\n",
    "    \"./media/video_2_processed.mp4\", \n",
    "    cv2.VideoWriter_fourcc('X','V','I','D'),\n",
    "    video_fps,\n",
    "    (video_width,video_height))\n",
    "\n",
    "# Define input dimensions\n",
    "input_width = 1920\n",
    "input_height = 1080\n",
    "\n",
    "# Process the video frame by frame\n",
    "while cap.isOpened():\n",
    "    \n",
    "    # Read a frame from the video capture\n",
    "    ret,img = cap.read()\n",
    "    \n",
    "    # If no frame is returned (end of video), exit the loop\n",
    "    if img is None:\n",
    "        break\n",
    "    \n",
    "    # Make a copy of the frame for processing\n",
    "    img2 = img.copy()\n",
    "    \n",
    "    # Initialize lists for each frame    \n",
    "    violate=[]\n",
    "    centroids = []\n",
    "    coordinates_people = [[(),(),]]\n",
    "    people_too_close = []\n",
    "    bounding_boxes = []\n",
    "    \n",
    "    # Use YOLOv8 to detect people in the frame\n",
    "    results = model.predict(img)\n",
    "    result = results[0]\n",
    "    not_violate = []\n",
    "\n",
    "    # Append coordinates of detected people\n",
    "    for box in result.boxes:\n",
    "        # Get the class ID and confidence score\n",
    "        class_id = box.cls[0].item()\n",
    "        conf = round(box.conf[0].item(), 2)\n",
    "        \n",
    "        # Get the bounding box coordinates and round them to integers\n",
    "        x0, y0, x1, y1 = [round(i) for i in box.xyxy[0].tolist()]\n",
    "        \n",
    "        # Check if the confidence is above 0.4 and the detected object is a person\n",
    "        if conf >= 0.4 and class_id == 0:\n",
    "            \n",
    "            # Append the bounding box coordinates to the list\n",
    "            bounding_boxes.append([x0, y0, x1, y1])\n",
    "            \n",
    "            # Calculate the height of the bounding box\n",
    "            height = int((y1 - y0))\n",
    "            \n",
    "            # Calculate the safe distance based on the height\n",
    "            safe_distance = int((2 * height )/ 1.7)\n",
    "            \n",
    "            # Append the coordinates and safe distance to the list\n",
    "            coordinates_people.append([ (x0, y0) ,   (x1, y1) ,   safe_distance   ])\n",
    "            \n",
    "            # Calculate the centroid of the bounding box\n",
    "            center_x = int((x0+x1) / 2)\n",
    "            center_y = int((y0+y1) / 2)\n",
    "            \n",
    "            # Append the centroid to the list\n",
    "            centroids.append((center_x, center_y))\n",
    "    \n",
    "    # Remove the initial placeholder\n",
    "    coordinates_people.pop(0)\n",
    "    \n",
    "\n",
    "    # If at least 2 people are detected in the frame, check if the distance between them is safe\n",
    "    if len(coordinates_people) >= 2:\n",
    "        \n",
    "        # Calculate the pairwise Euclidean distances between the centroids of detected people\n",
    "        euclidean_distance = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "        \n",
    "        # Compare the euclidean distances to the safe distance for each pair of people\n",
    "        for i in range(len(coordinates_people)):\n",
    "            for j in range (i+1,len(coordinates_people)):\n",
    "                \n",
    "                if euclidean_distance[i,j] < coordinates_people[i][2] or euclidean_distance[i,j] < coordinates_people[j][2]:                                                                                                                       \n",
    "                    if [coordinates_people[i][0],coordinates_people[i][1]] not in people_too_close: people_too_close.append([coordinates_people[i][0], coordinates_people[i][1]])   \n",
    "                    if [coordinates_people[j][0],coordinates_people[j][1]] not in people_too_close: people_too_close.append([coordinates_people[j][0], coordinates_people[j][1]])\n",
    "\n",
    "                        \n",
    "        # Detect faces only in people who are too close to each other\n",
    "        for idx in range(len(people_too_close)):\n",
    "            \n",
    "            # Extract the region of interest (ROI) for each person who is too close\n",
    "            person_rgb = img[people_too_close[idx][0][1]:people_too_close[idx][1][1], people_too_close[idx][0][0]:people_too_close[idx][1][0]]\n",
    "            \n",
    "            # Draw a rectangle around the detected person\n",
    "            cv2.rectangle(img2, people_too_close[idx][0], people_too_close[idx][1], (0,255,255), 3)\n",
    "            \n",
    "            # Apply Gaussian blur to the region of interest (ROI) to reduce noise\n",
    "            person_rgb = cv2.GaussianBlur(person_rgb, (5,5), cv2.BORDER_DEFAULT) \n",
    "            \n",
    "            # Detect faces within the ROI\n",
    "            detections = detector.detect(person_rgb, shrink=1.0)\n",
    "            \n",
    "            # Initialize variables to find the largest face detected\n",
    "            test_area = 0\n",
    "            biggest_face = ((0,0),(0,0))\n",
    "            faces=[]\n",
    "            face_test = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(len(detections)):\n",
    "                \n",
    "                # Convert the detection to a numpy array\n",
    "                detection = np.array(detections[i]) \n",
    "                \n",
    "                # Replace any negative values in the detection with 0\n",
    "                detection = np.where(detection<0,0,detection)  \n",
    "                \n",
    "                # Calculate the coordinates of the face bounding box relative to the original image\n",
    "                face_x0 = people_too_close[idx][0][0] + int(detection[0])    \n",
    "                face_x1 = people_too_close[idx][0][0] + int(detection[2])\n",
    "                face_y0 = people_too_close[idx][0][1] + int(detection[1])\n",
    "                face_y1 = people_too_close[idx][0][1] + int(detection[3])\n",
    "                \n",
    "                # Calculate the area of the face bounding box\n",
    "                face_area = int((face_x0 - face_x1) * (face_y0 - face_y1))\n",
    "                \n",
    "                 # Append the face coordinates and area to the list of faces\n",
    "                faces.append([ (face_x0, face_y0), (face_x1, face_y1), face_area])\n",
    "\n",
    "                # Draw a rectangle around each detected face\n",
    "                cv2.rectangle(img, faces[i][0], faces[i][1], (55,0,255), 3)\n",
    "            \n",
    "            \n",
    "            # Find the largest face detected\n",
    "            for j in range(len(faces)):\n",
    "                if faces[j][2] > test_area:\n",
    "                    biggest_face=(faces[j][0], faces[j][1])\n",
    "                    test_area = faces[j][2]\n",
    "\n",
    "            # Append the largest face to the face_test list\n",
    "            face_test.append(biggest_face)\n",
    "            \n",
    "            \n",
    "            for var in range(len(face_test)):\n",
    "                try:\n",
    "                    # Draw a rectangle around the largest face\n",
    "                    cv2.rectangle(img2, (face_test[var][0]), (face_test[var][1]), (255,255,0), 3)\n",
    "                    \n",
    "                    # Extract the face region and preprocess it for mask detection\n",
    "                    face_rgb = img[face_test[var][0][1]:face_test[var][1][1], face_test[var][0][0]:face_test[var][1][0],::-1]\n",
    "                    face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)    \n",
    "                    face_arr = pre_mobile(face_arr)   \n",
    "                    face_arr = cv2.GaussianBlur(face_arr, (5,5), cv2.BORDER_DEFAULT)\n",
    "                    face_arr = np.expand_dims(face_arr, axis=0)           \n",
    "                    \n",
    "                    # Predict if the person is wearing a mask\n",
    "                    check_if_mask = mask_detector.predict(face_arr)\n",
    "                    \n",
    "                    if check_if_mask[0][0] > 0.89:\n",
    "                        violate.append(people_too_close[idx])\n",
    "                        \n",
    "                        # Draw a circle on the detected face without a mask\n",
    "                        cv2.circle(img2, face_test[var][0], 3, (0, 255,0),2)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    \n",
    "    # Draw red bounding box if violating the rules\n",
    "    for m in range(len(violate)):\n",
    "        cv2.rectangle(img2, violate[m][0], violate[m][1], (0,0,255), 3)     \n",
    "    \n",
    "    # If a frame was successfully read write it to the output video stream\n",
    "    if ret:\n",
    "        out_stream.write(img2)\n",
    "    \n",
    "    # Display the processed frame in a window named \"Video\"\n",
    "    cv2.imshow(\"Video\", img2)\n",
    "    \n",
    "    # If 'q' was pressed, exit the loop\n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "        \n",
    "# Release the video capture object\n",
    "cap.release()    \n",
    "\n",
    "# Release the output video stream\n",
    "out_stream.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344d684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
