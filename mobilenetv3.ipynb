{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6239ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53048c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the argument parser and parse the arguments\n",
    "sys.argv = ['mobilenetv3.ipynb', '-d', 'dataset']\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
    "    help=\"path to input dataset\")\n",
    "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    "    help=\"path to output loss/accuracy plot\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str,\n",
    "    default=\"mask_detector.model\",\n",
    "    help=\"path to output face mask detector model\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbda1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the initial learning rate, number of epochs to train for, and batch size\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "217ff81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Python\\Anaconda\\Anaconda3\\Lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Print a message indicating that images are being loaded\n",
    "print(\"[INFO] loading images...\")\n",
    "\n",
    "# Grab the list of image paths\n",
    "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "\n",
    "# Initialize the lists to store image data and corresponding labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "# Loop over each image path\n",
    "for imagePath in imagePaths:\n",
    "    \n",
    "    # Extract the class label from the directory name (second last part of the path)\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    \n",
    "    # Load the input image, resize it to 224x224 pixels, and preprocess it\n",
    "    image = load_img(imagePath, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    # Append the processed image to the data list\n",
    "    data.append(image)\n",
    "    \n",
    "    # Append the corresponding label to the labels list\n",
    "    labels.append(label)\n",
    "    \n",
    "# Convert the lists of data and labels to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13ab7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# Partition the data into training and testing splits\n",
    "# Use 80% of the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "    test_size=0.20, stratify=labels, random_state=42)\n",
    "\n",
    "# Construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38a38d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MobileNetV3Large network, ensuring the fully connected (FC) layer sets are left off\n",
    "baseModel = MobileNetV3Large(weights=None, include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# Load the weights from the local file\n",
    "weights_path = 'models/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5'\n",
    "baseModel.load_weights(weights_path)\n",
    "\n",
    "# Construct the head of the model that will be placed on top of the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "\n",
    "# Place the head FC model on top of the base model (this will become the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "\n",
    "# Loop over all layers in the base model and freeze them so they will not be updated\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5a92bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training head...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 1s/step - accuracy: 0.6676 - loss: 0.6431 - val_accuracy: 0.8242 - val_loss: 0.3696\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 138ms/step - accuracy: 0.8567 - loss: 0.3507 - val_accuracy: 0.9375 - val_loss: 0.1964\n",
      "Epoch 3/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 1s/step - accuracy: 0.8820 - loss: 0.2792 - val_accuracy: 0.9531 - val_loss: 0.1383\n",
      "Epoch 4/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 135ms/step - accuracy: 0.9148 - loss: 0.2196 - val_accuracy: 0.9492 - val_loss: 0.1306\n",
      "Epoch 5/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 1s/step - accuracy: 0.9213 - loss: 0.2006 - val_accuracy: 0.9688 - val_loss: 0.0863\n",
      "Epoch 6/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 139ms/step - accuracy: 0.9291 - loss: 0.1885 - val_accuracy: 0.9694 - val_loss: 0.0812\n",
      "Epoch 7/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 1s/step - accuracy: 0.9307 - loss: 0.1793 - val_accuracy: 0.9746 - val_loss: 0.0728\n",
      "Epoch 8/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 138ms/step - accuracy: 0.9498 - loss: 0.1517 - val_accuracy: 0.9792 - val_loss: 0.0645\n",
      "Epoch 9/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 1s/step - accuracy: 0.9556 - loss: 0.1334 - val_accuracy: 0.9841 - val_loss: 0.0559\n",
      "Epoch 10/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 134ms/step - accuracy: 0.9612 - loss: 0.1228 - val_accuracy: 0.9870 - val_loss: 0.0487\n",
      "Epoch 11/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 1s/step - accuracy: 0.9659 - loss: 0.1015 - val_accuracy: 0.9896 - val_loss: 0.0423\n",
      "Epoch 12/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 135ms/step - accuracy: 0.9704 - loss: 0.0912 - val_accuracy: 0.9918 - val_loss: 0.0378\n",
      "Epoch 13/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 1s/step - accuracy: 0.9728 - loss: 0.0867 - val_accuracy: 0.9921 - val_loss: 0.0362\n",
      "Epoch 14/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 136ms/step - accuracy: 0.9753 - loss: 0.0819 - val_accuracy: 0.9942 - val_loss: 0.0315\n",
      "Epoch 15/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 997ms/step - accuracy: 0.9768 - loss: 0.0784 - val_accuracy: 0.9953 - val_loss: 0.0297\n",
      "Epoch 16/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 135ms/step - accuracy: 0.9776 - loss: 0.0767 - val_accuracy: 0.9922 - val_loss: 0.0291\n",
      "Epoch 17/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 1s/step - accuracy: 0.9646 - loss: 0.1042 - val_accuracy: 1.0000 - val_loss: 0.0243\n",
      "Epoch 18/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 142ms/step - accuracy: 0.9672 - loss: 0.0904 - val_accuracy: 0.9961 - val_loss: 0.0244\n",
      "Epoch 19/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 1s/step - accuracy: 0.9710 - loss: 0.0836 - val_accuracy: 0.9883 - val_loss: 0.0340\n",
      "Epoch 20/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 135ms/step - accuracy: 0.9748 - loss: 0.0711 - val_accuracy: 0.9922 - val_loss: 0.0270\n"
     ]
    }
   ],
   "source": [
    "# Print a message indicating that the model is being compiled\n",
    "print(\"[INFO] compiling model...\")\n",
    "\n",
    "# Initialize the Adam optimizer with the specified learning rate\n",
    "opt = Adam(learning_rate=INIT_LR)\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and the Adam optimizer\n",
    "# Also specify that we want to track accuracy during training\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# Print a message indicating that the head of the network is being trained\n",
    "print(\"[INFO] training head...\")\n",
    "\n",
    "# Train the head of the network\n",
    "# Use the data augmentation generator to provide augmented training data\n",
    "# Specify the number of steps per epoch and validation steps\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    validation_data=(testX, testY),\n",
    "    validation_steps=len(testX) // BS,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6ec081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 556ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   with_mask       0.99      1.00      0.99      138\n",
      "without_mask       1.00      0.99      0.99      138\n",
      "\n",
      "    accuracy                           0.99      276\n",
      "   macro avg       0.99      0.99      0.99      276\n",
      "weighted avg       0.99      0.99      0.99      276\n",
      "\n",
      "[INFO] saving mask detector model...\n"
     ]
    }
   ],
   "source": [
    "# Print a message indicating that the network is being evaluated\n",
    "print(\"[INFO] evaluating network...\")\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predIdxs = model.predict(testX, batch_size=BS)\n",
    "\n",
    "# For each image in the testing set, find the index of the label with the highest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "# Print a classification report showing the performance of the model\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs, target_names=lb.classes_))\n",
    "\n",
    "# Print a message indicating that the mask detector model is being saved\n",
    "print(\"[INFO] saving mask detector model...\")\n",
    "\n",
    "# Save the trained model to disk\n",
    "model.save(\"models/mask_detector_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f50f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
